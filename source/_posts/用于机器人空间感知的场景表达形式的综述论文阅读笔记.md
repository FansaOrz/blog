---
layout: post
title: 机器人空间感知的场景表达形式的综述论文阅读笔记
date: 2025-02-07 14:30:23
tags: [视觉重建, SDF, SLAM, 空间感知, NeRF, 3DGS, 机器学习, MLP, 深度学习, 论文解读]
math: true
categories: 空间感知
excerpt: "关于场景表示形式的综述论文阅读笔记"
---

# 相关连接
* [论文链接](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/707302/Scene_Representations_for_Robotic_Spatial_Perception.pdf?sequence=9&isAllowed=y)
* https://github.com/3D-Vision-World/awesome-NeRF-and-3DGS-SLAM
# 空间感知基础
* 构建场景就是用原始的传感器数据增量式的构建机器人周边的环境，同时估计机器人自身位置。并且机器人可以在这个地图上做自主导航，并且和周围环境进行交互。作者将这类地图分为三个抽象的层次：
  + metric：场景的几何信息
  + semantic：分辨物体的类别或者整个场景的语义信息
  + topological：场景和物体之间，以及物体之间的相互关系

  <p align="center">{% asset_img spatial_perception.png %}</p>

* 如果要设计一个机器人空间感知系统，或者是一个空间的表示形式时，作者提供了一些需要考虑的关键要素：
  + **Metric accuracy（几何精度）**：除了几何精度以外，还需要保持地图和真实环境的一致性，例如空间中的每个位置，都只有一个地图中的点与其对应。
  + **Semantic richness（语义丰富度）**：空间中物体的实例分类，甚至是场景的语义理解，例如室内场景，要把不同的房间区分开。
  + **Scalability（可扩展性）**：场景的表示形式要能够适应不同大小的场景，室内小场景，室外复杂环境，甚至整个城市的表达。因此当新数据进来时，要能实现实时的推理和更新，并且只动态的更新新观测到的部分。需要做到内存和计算资源的管理以及精度和丰富度的平衡。
  + **Efficiency（效率）**：与上一点类似，要能实时运行。
  + **Robustness（鲁棒性）**：不仅能处理传感器的噪声，还要能处理环境的变化，甚至部分系统故障时也能正常运行。
  + **Long-term persistence（长期持久性）**：在机器人的长期运行中，可以持续的更新和维护地图。
  + **Multi-modal integration（多模态融合）**：能接纳多传感器的数据，例如相机，LiDAR，声呐，触觉传感器等。
  + **Interpretablity and usability（可解释性和可用性）**：这个场景既能让机器人理解，又能让人类理解。
# 场景表示形式
* 主要分为三个大类：metric，metric-semantic和metric-semantic-topological。以下是一个概览：

<p align="center">{% asset_img timeline.png %}</p>

## Metric Representations（只包括几何信息）

* 这一类就是纯做SLAM了。

### Feature-based representations（基于稀疏特征的表示形式）

* 传感器捕捉场景中的特征点或者Landmark，然后用这些特征点来构建地图。视觉SLAM中主要是用这种方法，构建稀疏特征点的地图，例如ORB-SLAM。也有一些利用深度学习的方法提取特征，或者提取空间中的线面特征的VSLAM方法，在LiDAR-based的SLAM中，LOAM及其变种也都是特征点法。

### Dense (and semi-dense) point-based representations（密集点云表示形式）

* 基于RGBD和LiDAR的SLAM主要生成的是稠密点云，其主要目标是做状态估计，根据输入数据做匹配，计算当前位置和姿态。也有一些用神经网络代替传统SLAM的后端，直接估计单目的深度实现稠密点云的构建，例如[DVSO](https://github.com/SenZHANG-GitHub/DVSO)和[D3VO](https://github.com/as821/D3VO)。这类方法对于单目和的尺度估计和无纹理区域的深度估计有比较大的提升。

### Surface mesh representations（表面网格表示形式）

* 实时生成mesh，主要是从一些基础的数据结构中转换而来的，例如基于LiDAR的[ImMesh](https://github.com/hku-mars/ImMesh)，很少有实时生成mesh同时也动态的优化mesh的方法，例如基于Lidar生成mesh的[puma](https://github.com/PRBonn/puma)以及[SLAMesh](https://github.com/lab-sun/SLAMesh)

<p align="center">{% asset_img slamesh.png SLAMesh%}</p>

### Classical volumetric representations（传统体积表示形式）

* 这种体积表示的方法，主要目标是既表示3D的几何信息，又把free space表示出来，这种可能会更适合机器人做自主导航和规划。比较常用的方法是
  + **occupancy maps（占据栅格）**：明确的表示free space和occupied space。
  + **Signed Distance Fields（SDF）**：每个点保存其到最近表面的正交距离（有正有负）。
* 与占据栅格相关的方法中，比较经典的方法有[OctoMap](https://octomap.github.io/)，其将空间中的点按照不同的分辨率保存，加快了对空间栅格的访问。其他的对OctoMap的访问做加速的框架有[UFOMap](https://github.com/UnknownFreeOccupied/ufomap)以及利用GPU+ray tracing的[OctoMap-RT](https://github.com/heajungmin/OctoMap-RT)，比OctoMap快三十几倍。

<p align="center">{% asset_img ufomap.png UFOMap%}</p>

* 由于栅格的离散特性，使得这种建模方法会有不准确的地方，因此后续提出了[NDT(Normal Distributions Transform)](https://github.com/cogsys-tuebingen/cslibs_ndt)的方案。NDT将每个栅格建模成一个局部的高斯分布，将环境可以实现分段的连续表示。因此在比较大的体素尺寸时也可以达到比较好的建模精度，并且进一步促进了类似的Hilbert maps和[Gaussian Mixture Models(GMMs)](https://github.com/mit-lean/GMMap)的发展。

* 另一种基于TSDF的方法由于可以实现亚体素的分辨率以及实现传感器的去噪，在SLAM中应用也比较广泛。比较有名的[KinectFusion](https://github.com/ParikaGoel/KinectFusion)。为了更好的支持轨迹规划，很多框架目标是在动态增长的地图上实时构建完整的SDF，因为**这类方法允许从空间中任何一个点有效的查询到障碍物的距离和梯度。**
  + 例如，有遵循voxel-hashing的[Volblox](https://github.com/ethz-asl/voxblox)，它整合传感器的投影后的TSDF逐步构建SDF map。

<p align="center">{% asset_img Volblox.png Volblox%}</p>

  + [Voxfield](https://github.com/VIS4ROB-lab/voxfield)引入了一个不需要投影的TSDF公式来实现更高精度的SDF并实现了高效的计算。

<p align="center">{% asset_img Voxfield.png Voxfield%}</p>

  + 基于GPU做加速的[nvblox](https://github.com/nvidia-isaac/nvblox)

<p align="center">{% asset_img nvblox.gif nvblox%}</p>

  + 以及基于多分辨率构建SDF以节省内存的方法：Adaptive-resolution octree-based volumetric SLAM和Efficient Octree-Based Volumetric SLAM Supporting Signed-Distance and Occupancy Mapping

### Neural implicit representations（神经隐式表示）

* 基于深度学习的模型来做场景纹理和几何信息的隐式表达。
* 早期的工作主要是用深度autoencoders来自动发觉高维图像的高级、紧凑表示。[CodeSLAM](https://github.com/silviutroscot/CodeSLAM)在VSLAM中，将观察到的图片编码成紧凑且可优化的表示，且这种编码形式包含了稠密场景的基本信息。[DeepFactors](https://github.com/jczarnowski/DeepFactors?tab=readme-ov-file)将这种隐式表达整合到一个全特征，基于关键帧的SLAM框架中。

<p align="center">{% asset_img deepfactors.gif deepfactors%}</p>

* 最近3年，[NeRF](https://github.com/bmild/nerf)方法兴起，也带起了一批基于NeRF的SLAM方法。基于自监督学习的方法，隐式的学习并建模三维场景的结构和纹理并实现新视角的合成，这个方法在RGB-D SLAM领域被迅速的采用。[iMap](https://github.com/SrinjaySarkar/imap-Implicit-Mapping-and-Positioning-in-Real-Time)使用MLP将3D坐标映射到颜色和体积密度，而无需考虑观察方向等问题。尽管这种方法可以实现紧凑且连续的场景建模，但他让3D表面过于平滑，并且有灾难性遗忘、推理速度过慢等问题。考虑到以上问题，MeSLAM和NISB-Map采用了多个MLP网络，分别代表场景中的不同部分。
  + [NICE-SLAM](https://github.com/cvg/nice-slam)采用了基于栅格的表示方法，其引入了固定大小，coarse-to-fine的特征栅格，使其在大型的室内场景中也可以重建出更好的细节。

<p align="center">{% asset_img niceslam.gif NICE-SLAM%}</p>

  + [Vox-Fusion](https://github.com/zju3dv/Vox-Fusion)采用八叉树来存储grid embeddings，使其在场景扩展时可以动态的分配新的体素。

<p align="center">{% asset_img voxfusion.jpg Vox-Fusion%}</p>

  + [Co-SLAM](https://github.com/HengyiWang/Co-SLAM)将one-blob编码提供的平滑度与[多分辨率的hash-based的特征栅格](https://github.com/NVlabs/instant-ngp)的快速收敛与局部细节优势结合起来，进一步提高了Vox-Fusion的效果。

<p align="center">{% asset_img coslam.png Co-SLAM%}</p>

  + [ESLAM](https://github.com/idiap/ESLAM)用多尺度、坐标轴对齐的特征平面代替特征栅格，将场景增长的规模从三次方缩小到二次方。

<p align="center">{% asset_img eslam.jpg ESLAM%}</p>

  + 除了基于栅格的表达的方法外，基于混合神经点的表达方法也有一些，例如[Point-SLAM](https://github.com/eriksandstroem/Point-SLAM)和[Loopy-SLAM](https://github.com/eriksandstroem/Loopy-SLAM)。这种点表示的方法，和栅格方法不同，无需预先定义栅格的分辨率，因此对于内存的使用也更有效，并且可以捕捉更多的细节。整体看下来，基于点的方法重建的效果更佳。

<p align="center">{% asset_img Point-SLAM.gif Point-SLAM%}</p>
<p align="center">{% asset_img loopyslam.gif Loopy-SLAM%}</p>

* 以上都是RGBD相机的稠密重建，对于RGB相机的重建也有一些工作。iMODE用ORB-SLAM2实时计算相机的pose，并优化MLP用于神经场表示。类似的工作还有[Orbeez-SLAM](https://github.com/MarvinChung/Orbeez-SLAM)和[NeRF-SLAM](https://github.com/ToniRV/NeRF-SLAM)都是先用VSLAM获得相机的位姿，然后用[Instant-NGP](https://github.com/NVlabs/instant-ngp)做实时重建。[NICER-SLAM](https://github.com/cvg/nicer-slam)引入了一个端到端的网络，可以同时优化相机pose和基于SDF和颜色的分层神经隐式映射场。

<p align="center">{% asset_img nicerslam.png NICER-SLAM%}</p>

* 在基于LiDAR的重建中，[NeRF-LOAM](https://github.com/JunyuanDeng/NeRF-LOAM)在构建神经表示的同时，优化LiDAR的位姿，并且采用八叉树的结构中采用动态的体素嵌入，可以有效的捕捉局部几何信息。[LONER](https://github.com/umautobots/LONER)正相反，它使用ICP估计LiDAR的位姿，并用具有分层特征网格编码的MLP来表示场景。[PIN-SLAM](https://github.com/PRBonn/PIN_SLAM?tab=readme-ov-file)利用基于点的隐式神经图表示与“无需对应点”的“点到隐式模型”的配准方法结合，实现姿态估计，并且允许在全局姿态调整期间连续变形。

<p align="center">{% asset_img loner.png LONER%}</p>
<p align="center">{% asset_img pinslam.png PIN-SLAM%}</p>

### Explicit radiance field representations（显式辐射场表示）

* 显示的表示则主要是基于[3DGS](https://github.com/graphdeco-inria/gaussian-splatting)的方法，其比NeRF速度更快，同时渲染质量也很好。
  + 在RGB-D传感器背景下，以3DGS来表示场景的SLAM方法有[GS-SLAM](https://github.com/yanchi-3dv/diff-gaussian-rasterization-for-gsslam)和[SplaTAM](https://github.com/spla-tam/SplaTAM)。

<p align="center">{% asset_img gsslam.png GS-SLAM%}</p>
<p align="center">{% asset_img splatam.gif SplaTAM%}</p>

  + 单目相机的SLAM重建方法有[Gaussian Splatting SLAM](https://github.com/muskie82/MonoGS)
  + 以及针对LiDAR-Inertial-Vision融合的[LIV-GaussMap](https://github.com/sheng00125/LIV-GaussMap)

<p align="center">{% asset_img livgaussmap.png LIV-GaussMap%}</p>

* 以上这些方法都**很依赖初始化**，以及**对未观察到的区域的重建效果较差**，并且3DGS为了实现场景重建的精度，使用了大量的高斯基元，因此对内存的占用也比较大。

## Metric-Semantic Representations（同时包含几何信息和语义信息）

* 与上面介绍的方法不同，这类方法给场景的几何信息增加了语义标签（例如：物体的种类），这类方法主要受益于object-detection，classification和scene understanding的深度学习方法的快速发展。
* 这类场景表示主要分为4类：object-centric，semantically-labeled, panptic和open-set representations.

### Object-centric representations（物体为中心的表示）
- 这类方法主要对目标物体有区分和重建，其他部分的语义信息则会直接忽视。早期的方法是有一个3D模型库，并且要求物体的几何形状和库里的模型保持一致。受益于CNN的发展，对于物体的类别、姿态和尺寸都可以很好的估计出来，因此在基于lanemark的SLAM方法中用的很多。例如用dual quadric fromulation（对偶二次曲面）方法将物体建模成3D椭圆的QuadricSLAM，以及用物体的BBOX计算残差的[CubeSLAM](https://github.com/shichaoy/cube_slam)。

<p align="center">{% asset_img cubeslam.png CudeSLAM%}</p>

- 目前基于RGB-D的场景稠密重建方法发展速度更快。Fusion++用Mask R-CNN预测image中物体的mask，并逐步构建3D物体模型，并把它作为pose graph中可优化的landmark。与其类似，[Voxblox++](https://github.com/ethz-asl/voxblox-plusplus)把Mask R-CNN和增量几何场景分割方法结合起来，实现了一个有物体语义实例的建图方法，可以生成新观测到的和已经有的物体的位姿和形状。

<p align="center">{% asset_img voxblox.png Voxblox++%}</p>

- 后续的《Volumetric Instance-Level Semantic Mapping Via Multi-View 2D-to-3D Label Diffusion》使用Mask R-CNN预测提取3D物体的instance，这些实例使用标签扩展方案集成到全局的稠密地图后能被进一步的细化，并且将这类方法更好的适用于Vision-LiDAR的传感器组合。
- 以上方法主要使用点云和TSDF来表示场景，[vMAP](https://github.com/kxhit/vMAP)提供了一个新思路，为每个物体，设置一个专用的MLP表示，最终实现object-level的稠密建图。这种方法有助于构建水密(watertight)和完整的物体模型，即使实时的RGB-D数据有遮挡。类似的，[RO-MAP](https://github.com/XiaoHan-Git/RO-MAP)从单目相机的RGB输入中，结合NeRF模型实现了轻量级的object-centric的SLAM方法。他为每个物体单独训练一个NeRF模型，实现了在没有深度先验信息的情况下，实时重建物体。

<p align="center">{% asset_img vmap.png vMAP%}</p>

### Semantically-labeled representations（语义标签的表示）
- 这类方法是给构建的场景中的每个点、surfel或者voxel体素赋予一个语义标签或者分布概率标签。比较有代表性的工作有基于RGB-D的SemanticFusion，他首次用CNN推断每个像素的类别的概率分布，然后用贝叶斯更新方法聚合到surfel的地图上。类似的[Kimera](https://github.com/MIT-SPARK/Kimera)同时将逐帧的语义分割结果、IMU测量和可选择的深度估计集成到一个mesh表示中，实现了实时的语义SLAM。[SuMa++](https://github.com/PRBonn/semantic_suma)和SA-LOAM则是基于LiDAR的语义分割SLAM方法。

<p align="center">{% asset_img kimera.png vMAP%}</p>
<p align="center">{% asset_img suma++.png SuMa++%}</p>

- 最近做的比较多的则是基于RGB-D传感器，以神经场的形式表示场景。[SNI-SLAM](https://github.com/IRMVLab/SNI-SLAM)使用神经隐式表示，分层语义编码实现多层次的场景理解，并用cross-attention机制协同整合外观，几何和语义特征。

<p align="center">{% asset_img snislame.png SNI-SLAM%}</p>

- [SGS-SLAM](https://github.com/ShuhongLL/SGS-SLAM)和[SemGauss-SLAM
](https://github.com/IRMVLab/SemGauss-SLAM)正相反，都是基于3DGS的语义稠密SLAM方法。SGS-SLAM是将语义信息作为RGB的额外一个通道，并在关键帧处优化语义分割点。SemGauss-SLAM则是给3DGS扩展了语义编码，是其能获得更高精度的3D语义分割。

<p align="center">{% asset_img sgsslam.png SGS-SLAM%}</p>
<p align="center">{% asset_img semgauss-slam.png SemGauss-SLAM%}</p>

### Panoptic representations（全景式表示）
- 上述方法无法分割出同一类别的实例信息，为了解决这个问题，所谓的Panoptic representations（全景表示）就是结合背景区域的标签，同时单独分割和识别任意前景对象。《Real-time Progressive 3D Semantic Segmentation for Indoor Scenes》使用纯语义分割策略，然后在场景中聚类实现实例分割。这种聚类的方法纯基于几何信息，因此分割出来的实例肯定不准。最近基于RGB-D的建图方法，将深度学习的全景分割和传统的体积表示形式结合了起来。[Panoptic Multi-TSDFs](https://github.com/ethz-asl/panoptic_mapping)实现了多分辨率的体素建图，他将整个环境建模型一组submap，每个代表一个实例物体或者一块背景，并且每块都有自己的分辨率，**这样就可以做到动态环境的更新。**

<p align="center">{% asset_img panoptic_mapping.png Panoptic_Multi-TSDFs%}</p>

- 除了传统的体积表示，[PanoRecon](https://github.com/Riser6/PanoRecon)还有[EPRecon](https://github.com/zhen6618/EPRecon)还提出了一种可以实时增量式的进行3D几何重建以及3D全景分割的框架。同时还有一些基于神经辐射场的方法，将背景和前景解耦，但是耗时比较长，因此只能离线运行。

<p align="center">{% asset_img eprecon.png EPRecon%}</p>

### Open-set representations（开放集表示）

- 大多数构建语义地图的方法，都局限于目前的闭集，只有预训练中见过的类别才能被识别到。因此为了应对这个问题，最近出现了由基础模型驱动的开放集表示的方法。这些新颖的框架能够将开放集特征融合到3D地图中，从而允许以后灵活的查询和解释。这一新兴领域最具代表性的例子之一是[ConceptFusion](https://github.com/concept-fusion/concept-fusion)，它证明了像素对齐的开放集特征可以通过传统的SLAM和多视图融合方法融合到3D地图中，从而实现有效的zero-shot空间推理。

<p align="center">{% asset_img conceptfusion.gif ConceptFusion%}</p>

## Metric-Semantic-Topological Representations（同时包含几何信息、语义信息和拓扑信息）
- 对于机器人导航来说，如果要理解并执行人类的指令，比如：“去厨房把我的咖啡杯拿来”，需要构建一个了解实例之间的关系并在不同层级构建关联的模型。目前已经有一些工作构建“3D Scene Graphs”来描述空间更丰富的信息，比如几何、位置、语义和拓扑关系。这种关系可以是“flat扁平”的，如果只关注单一层面，例如只是物体层；也可以是“hierarchical多层”的，比如从大到小的地点层面，建筑层面，房间层面，物体层面等。
- [3DSSG](https://github.com/ShunChengWu/3DSSG)（离线运行）和[SceneGraphFusion](https://github.com/ShunChengWu/SceneGraphFusion)（增量处理）用学习的方法，从不知道类别的，聚类后的点云中，推断物体之间的空间位置关系；

<p align="center">{% asset_img SceneGraphFusion.png SceneGraphFusion%}</p>

- [Hydra](https://github.com/MIT-SPARK/Hydra)实现了可以实时运行的多层3D sence graph。它构建机器人周围的Euclidean Signed Distance Field(ESDF)并增量式的将其转成语义3D mesh，并且可以提取出场地和房间。

<p align="center">{% asset_img Hydra.png Hydra%}</p>

- 最近的[concept-graphs](https://github.com/concept-graphs/concept-graphs)和[Clio](https://github.com/MIT-SPARK/Clio)也是类似的构建单层和多层的语义拓扑关系图。

<p align="center">{% asset_img Hydra.png Hydra%}</p>
<p align="center">{% asset_img Clio.png Clio%}</p>

## Dynamic Scene Representations（动态场景表示）
- 历史上，绝大多数SLAM和空间感知框架都假设世界是静态的。然而，现实世界的环境通常是动态的，具有移动的人和物体，不断变化的条件（例如照明，天气）和不断变化的物体。

### Short-term dynamic（短期动态）

### Long-term dynamic（长期动态）

### Short- and long-term dynamic（短期和长期动态）

### Deformable environments（变形环境）