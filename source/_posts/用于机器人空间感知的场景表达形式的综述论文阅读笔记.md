---
layout: post
title: 机器人空间感知的场景表达形式的综述论文阅读笔记
date: 2025-02-07 14:30:23
tags: [视觉重建, SDF, SLAM, 空间感知, NeRF, 3DGS, 机器学习, MLP, 深度学习, 论文解读]
math: true
categories: 空间感知
excerpt: "关于场景表示形式的综述论文阅读笔记"
---

# 相关连接
* [论文链接](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/707302/Scene_Representations_for_Robotic_Spatial_Perception.pdf?sequence=9&isAllowed=y)
* https://github.com/3D-Vision-World/awesome-NeRF-and-3DGS-SLAM
# 空间感知基础
* 构建场景就是用原始的传感器数据增量式的构建机器人周边的环境，同时估计机器人自身位置。并且机器人可以在这个地图上做自主导航，并且和周围环境进行交互。作者将这类地图分为三个抽象的层次：
  + metric：场景的几何信息
  + semantic：分辨物体的类别或者整个场景的语义信息
  + topological：场景和物体之间，以及物体之间的相互关系

  <p align="center">{% asset_img spatial_perception.png %}</p>

* 如果要设计一个机器人空间感知系统，或者是一个空间的表示形式时，作者提供了一些需要考虑的关键要素：
  + **Metric accuracy（几何精度）**：除了几何精度以外，还需要保持地图和真实环境的一致性，例如空间中的每个位置，都只有一个地图中的点与其对应。
  + **Semantic richness（语义丰富度）**：空间中物体的实例分类，甚至是场景的语义理解，例如室内场景，要把不同的房间区分开。
  + **Scalability（可扩展性）**：场景的表示形式要能够适应不同大小的场景，室内小场景，室外复杂环境，甚至整个城市的表达。因此当新数据进来时，要能实现实时的推理和更新，并且只动态的更新新观测到的部分。需要做到内存和计算资源的管理以及精度和丰富度的平衡。
  + **Efficiency（效率）**：与上一点类似，要能实时运行。
  + **Robustness（鲁棒性）**：不仅能处理传感器的噪声，还要能处理环境的变化，甚至部分系统故障时也能正常运行。
  + **Long-term persistence（长期持久性）**：在机器人的长期运行中，可以持续的更新和维护地图。
  + **Multi-modal integration（多模态融合）**：能接纳多传感器的数据，例如相机，LiDAR，声呐，触觉传感器等。
  + **Interpretablity and usability（可解释性和可用性）**：这个场景既能让机器人理解，又能让人类理解。
# 场景表示形式
* 主要分为三个大类：metric，metric-semantic和metric-semantic-topological。以下是一个概览：

<p align="center">{% asset_img timeline.png %}</p>

## Metric Representations（只包括几何信息）

* 这一类就是纯做SLAM了。

### Feature-based representations（基于稀疏特征的表示形式）

* 传感器捕捉场景中的特征点或者Landmark，然后用这些特征点来构建地图。视觉SLAM中主要是用这种方法，构建稀疏特征点的地图，例如ORB-SLAM。也有一些利用深度学习的方法提取特征，或者提取空间中的线面特征的VSLAM方法，在LiDAR-based的SLAM中，LOAM及其变种也都是特征点法。

### Dense (and semi-dense) point-based representations（密集点云表示形式）

* 基于RGBD和LiDAR的SLAM主要生成的是稠密点云，其主要目标是做状态估计，根据输入数据做匹配，计算当前位置和姿态。也有一些用神经网络代替传统SLAM的后端，直接估计单目的深度实现稠密点云的构建，例如[DVSO](https://github.com/SenZHANG-GitHub/DVSO)和[D3VO](https://github.com/as821/D3VO)。这类方法对于单目和的尺度估计和无纹理区域的深度估计有比较大的提升。

### Surface mesh representations（表面网格表示形式）

* 实时生成mesh，主要是从一些基础的数据结构中转换而来的，例如基于LiDAR的[ImMesh](https://github.com/hku-mars/ImMesh)，很少有实时生成mesh同时也动态的优化mesh的方法，例如基于Lidar生成mesh的[puma](https://github.com/PRBonn/puma)以及[SLAMesh](https://github.com/lab-sun/SLAMesh)

<p align="center">{% asset_img slamesh.png SLAMesh%}</p>

### Classical volumetric representations（传统体积表示形式）

* 这种体积表示的方法，主要目标是既表示3D的几何信息，又把free space表示出来，这种可能会更适合机器人做自主导航和规划。比较常用的方法是
  + **occupancy maps（占据栅格）**：明确的表示free space和occupied space。
  + **Signed Distance Fields（SDF）**：每个点保存其到最近表面的正交距离（有正有负）。
* 与占据栅格相关的方法中，比较经典的方法有[OctoMap](https://octomap.github.io/)，其将空间中的点按照不同的分辨率保存，加快了对空间栅格的访问。其他的对OctoMap的访问做加速的框架有[UFOMap](https://github.com/UnknownFreeOccupied/ufomap)以及利用GPU+ray tracing的[OctoMap-RT](https://github.com/heajungmin/OctoMap-RT)，比OctoMap快三十几倍。

<p align="center">{% asset_img ufomap.png UFOMap%}</p>

* 由于栅格的离散特性，使得这种建模方法会有不准确的地方，因此后续提出了[NDT(Normal Distributions Transform)](https://github.com/cogsys-tuebingen/cslibs_ndt)的方案。NDT将每个栅格建模成一个局部的高斯分布，将环境可以实现分段的连续表示。因此在比较大的体素尺寸时也可以达到比较好的建模精度，并且进一步促进了类似的Hilbert maps和[Gaussian Mixture Models(GMMs)](https://github.com/mit-lean/GMMap)的发展。

* 另一种基于TSDF的方法由于可以实现亚体素的分辨率以及实现传感器的去噪，在SLAM中应用也比较广泛。比较有名的[KinectFusion](https://github.com/ParikaGoel/KinectFusion)。为了更好的支持轨迹规划，很多框架目标是在动态增长的地图上实时构建完整的SDF，因为**这类方法允许从空间中任何一个点有效的查询到障碍物的距离和梯度。**
  + 例如，有遵循voxel-hashing的[Volblox](https://github.com/ethz-asl/voxblox)，它整合传感器的投影后的TSDF逐步构建SDF map。

<p align="center">{% asset_img Volblox.png Volblox%}</p>

  + [Voxfield](https://github.com/VIS4ROB-lab/voxfield)引入了一个不需要投影的TSDF公式来实现更高精度的SDF并实现了高效的计算。

<p align="center">{% asset_img Voxfield.png Voxfield%}</p>

  + 基于GPU做加速的[nvblox](https://github.com/nvidia-isaac/nvblox)

<p align="center">{% asset_img nvblox.gif nvblox%}</p>

  + 以及基于多分辨率构建SDF以节省内存的方法：Adaptive-resolution octree-based volumetric SLAM和Efficient Octree-Based Volumetric SLAM Supporting Signed-Distance and Occupancy Mapping

### Neural implicit representations（神经隐式表示）

* 基于深度学习的模型来做场景纹理和几何信息的隐式表达。
* 早期的工作主要是用深度autoencoders来自动发觉高维图像的高级、紧凑表示。[CodeSLAM](https://github.com/silviutroscot/CodeSLAM)在VSLAM中，将观察到的图片编码成紧凑且可优化的表示，且这种编码形式包含了稠密场景的基本信息。[DeepFactors](https://github.com/jczarnowski/DeepFactors?tab=readme-ov-file)将这种隐式表达整合到一个全特征，基于关键帧的SLAM框架中。

<p align="center">{% asset_img deepfactors.gif deepfactors%}</p>

* 最近3年，[NeRF](https://github.com/bmild/nerf)方法兴起，也带起了一批基于NeRF的SLAM方法。基于自监督学习的方法，隐式的学习并建模三维场景的结构和纹理并实现新视角的合成，这个方法在RGB-D SLAM领域被迅速的采用。[iMap](https://github.com/SrinjaySarkar/imap-Implicit-Mapping-and-Positioning-in-Real-Time)使用MLP将3D坐标映射到颜色和体积密度，而无需考虑观察方向等问题。尽管这种方法可以实现紧凑且连续的场景建模，但他让3D表面过于平滑，并且有灾难性遗忘、推理速度过慢等问题。考虑到以上问题，MeSLAM和NISB-Map采用了多个MLP网络，分别代表场景中的不同部分。
  + [NICE-SLAM](https://github.com/cvg/nice-slam)采用了基于栅格的表示方法，其引入了固定大小，coarse-to-fine的特征栅格，使其在大型的室内场景中也可以重建出更好的细节。

<p align="center">{% asset_img niceslam.gif NICE-SLAM%}</p>

  + [Vox-Fusion](https://github.com/zju3dv/Vox-Fusion)采用八叉树来存储grid embeddings，使其在场景扩展时可以动态的分配新的体素。

<p align="center">{% asset_img voxfusion.jpg Vox-Fusion%}</p>

  + [Co-SLAM](https://github.com/HengyiWang/Co-SLAM)将one-blob编码提供的平滑度与[多分辨率的hash-based的特征栅格](https://github.com/NVlabs/instant-ngp)的快速收敛与局部细节优势结合起来，进一步提高了Vox-Fusion的效果。

<p align="center">{% asset_img coslam.png Co-SLAM%}</p>

  + [ESLAM](https://github.com/idiap/ESLAM)用多尺度、坐标轴对齐的特征平面代替特征栅格，将场景增长的规模从三次方缩小到二次方。

<p align="center">{% asset_img eslam.jpg ESLAM%}</p>

  + 除了基于栅格的表达的方法外，基于混合神经点的表达方法也有一些，例如[Point-SLAM](https://github.com/eriksandstroem/Point-SLAM)和[Loopy-SLAM](https://github.com/eriksandstroem/Loopy-SLAM)。这种点表示的方法，和栅格方法不同，无需预先定义栅格的分辨率，因此对于内存的使用也更有效，并且可以捕捉更多的细节。整体看下来，基于点的方法重建的效果更佳。

<p align="center">{% asset_img Point-SLAM.gif Point-SLAM%}</p>
<p align="center">{% asset_img loopyslam.gif Loopy-SLAM%}</p>

* 以上都是RGBD相机的稠密重建，对于RGB相机的重建也有一些工作。iMODE用ORB-SLAM2实时计算相机的pose，并优化MLP用于神经场表示。类似的工作还有[Orbeez-SLAM](https://github.com/MarvinChung/Orbeez-SLAM)和[NeRF-SLAM](https://github.com/ToniRV/NeRF-SLAM)都是先用VSLAM获得相机的位姿，然后用[Instant-NGP](https://github.com/NVlabs/instant-ngp)做实时重建。[NICER-SLAM](https://github.com/cvg/nicer-slam)引入了一个端到端的网络，可以同时优化相机pose和基于SDF和颜色的分层神经隐式映射场。

<p align="center">{% asset_img nicerslam.png NICER-SLAM%}</p>

* 在基于LiDAR的重建中，[NeRF-LOAM](https://github.com/JunyuanDeng/NeRF-LOAM)在构建神经表示的同时，优化LiDAR的位姿，并且采用八叉树的结构中采用动态的体素嵌入，可以有效的捕捉局部几何信息。[LONER](https://github.com/umautobots/LONER)正相反，它使用ICP估计LiDAR的位姿，并用具有分层特征网格编码的MLP来表示场景。[PIN-SLAM](https://github.com/PRBonn/PIN_SLAM?tab=readme-ov-file)利用基于点的隐式神经图表示与“无需对应点”的“点到隐式模型”的配准方法结合，实现姿态估计，并且允许在全局姿态调整期间连续变形。

<p align="center">{% asset_img loner.png LONER%}</p>
<p align="center">{% asset_img pinslam.png PIN-SLAM%}</p>

### Explicit radiance field representations（显式辐射场表示）

* 显示的表示则主要是基于[3DGS](https://github.com/graphdeco-inria/gaussian-splatting)的方法，其比NeRF速度更快，同时渲染质量也很好。
  + 在RGB-D传感器背景下，以3DGS来表示场景的SLAM方法有[GS-SLAM](https://github.com/yanchi-3dv/diff-gaussian-rasterization-for-gsslam)和[SplaTAM](https://github.com/spla-tam/SplaTAM)。

<p align="center">{% asset_img gsslam.png GS-SLAM%}</p>
<p align="center">{% asset_img splatam.gif SplaTAM%}</p>

  + 单目相机的SLAM重建方法有[Gaussian Splatting SLAM](https://github.com/muskie82/MonoGS)
  + 以及针对LiDAR-Inertial-Vision融合的[LIV-GaussMap](https://github.com/sheng00125/LIV-GaussMap)

<p align="center">{% asset_img livgaussmap.png LIV-GaussMap%}</p>

* 以上这些方法都**很依赖初始化**，以及**对未观察到的区域的重建效果较差**，并且3DGS为了实现场景重建的精度，使用了大量的高斯基元，因此对内存的占用也比较大。

## Metric-Semantic Representations（同时包含几何信息和语义信息）

* 与上面介绍的方法不同，这类方法给场景的几何信息增加了语义标签（例如：物体的种类），这类方法主要受益于object-detection，classification和scene understanding的深度学习方法的快速发展。
* 这类场景表示主要分为4类：object-centric，semantically-labeled, panptic和open-set representations.

### Object-centric representations（物体为中心的表示）
