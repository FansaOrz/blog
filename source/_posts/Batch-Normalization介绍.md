---
title: Batch Normalization介绍
date: 2024-8-18 13:42:53
tags: [机器学习, 基础知识, 数学, 神经网络, 深度学习]
math: true
categories: 机器学习
excerpt: BN层介绍
---

# BN层介绍
* BN层在2015年提出，[《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》](https://proceedings.mlr.press/v37/ioffe15.pdf)
* 他解决的主要问题是：深度神经网络随着网络深度加深，训练起来越困难，收敛越来越慢

## 为什么网络深度加深，训练起来越困难？

* 深度神经网络涉及到很多层的叠加，而每一层的参数更新都会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，是的高层需要不断去重新适应底层的参数更新。为了训练好模型，我们需要非常谨慎地去设定学习率、初始化权重以及尽可能细致的参数更新策略。

## BN层的作用

* BN的基本思想：
    - 深层神经网络在做非线性变换前的输入值，随着网络加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是**整体分布逐渐往非线性函数的取值区间的上下限两端靠近**，所以这导致反向传播时底层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因。
    - 而BN层就是通过一定的规范化手段，把神经网络任意神经元这个输入值的分布，强行拉回到均值为0，方差为1的标准正态分布。这样使得激活输入值落在非线性函数对输入比较敏感的区域中，网络的输入就不会很大，可以得到比较大的梯度，进而加快收敛速度。
# BN层的实现

<p align="center">{% asset_img bn.png BN层实现 %}</p>

1. 计算样本均值
2. 计算样本方差
3. 样本数据标准化处理
4. 进行平移和缩放处理。引入了$\gamma$和$\beta$两个参数，训练这两个参数。通过引入这两个可学习重构参数$\gamma$和$\beta$，让我们的网络可以学习恢复出原始网络所要学习的特征分布
# BN层的作用
1. 加快网络的训练和收敛速度
2. 控制梯度爆炸防止梯度消失
3. 防止过拟合

## 加快网络的训练和收敛速度

* 如果每层的数据分布都不一样，将会导致网络非常难收敛和训练，而如果把每层的数据都转换到一个标准正态分布，那么每层的数据分布就都一样了，这样网络就会更容易收敛和训练

## 控制梯度爆炸防止梯度消失

* 梯度消失：
    - 如果网络的激活输出很大，其对应的梯度就会很小，导致网络的学习速率就会很慢。
    - **以Sigmoid为例，假设网络中每层的学习梯度都小于最大值0.25，网络有n层，因为链式求导的原因，第一层的梯度将会小于$0.25^n$，所以学习速率相对来说会变得很慢，而对于网络的最后一层来说，只需要对自身求导一次，梯度很大， 学习速率就会很快，这导致在一个很深的网络中，浅层基本不学习，权重变化小，而后面几层网络一直在学习，后面的网络基本可以表征整个网络，就失去了深度的意义。**

* 梯度爆炸：
    - 第一层偏移量的梯度=激活层斜率1 x 权重1 x 激活层斜率2 x 权重2 x ... x 激活层斜率n x 权重n。假设激活层斜率都是0.25，权重为100，梯度就会指数增加。（使用BN层后权重的更新也不会很大）

## 防止过拟合

* 在网络的训练中，BN的使用使得一个minibatch中所有样本都被关联在了一起，因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取决于跟这个样本同属一个batch的其他样本，而每次网络都是随机取batch，这样就会使得整个网络不会朝这一个方向使劲学习。一定程度上避免了过拟合。

* BN层最重要的作用是加速网络的收敛速度，同时也让网络训练变得更加容易；另外调参过程也简单很多，对于初始化要求没那么高，而且可以使用大的学习率。
# 参考文档
* [神经网络中BN层的原理与作用](https://blog.csdn.net/weixin_42080490/article/details/108849715)
